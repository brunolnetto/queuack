#!/usr/bin/env python3
"""
Generator Streaming Example - Memory-Efficient Large Dataset Processing

This example demonstrates the benefits of using generator-based streaming
for processing large datasets with O(1) memory usage.

Scenario:
1. Extract: Generate 1M synthetic records (simulating large dataset)
2. Transform: Process records one at a time (memory efficient)
3. Load: Aggregate results and save

Shows comparison between:
- In-memory approach: Loads everything into RAM
- Generator approach: Streams data with constant memory usage
"""

import time
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from queuack import (
    DAG,
    DuckQueue,
    StreamReader,
    TaskContext,
    generator_task,
)


# ==============================================================================
# Approach 1: In-Memory (High Memory Usage)
# ==============================================================================

def in_memory_example():
    """Traditional in-memory approach - loads everything into RAM."""
    print("\n" + "="*70)
    print("Approach 1: In-Memory Processing (High Memory)")
    print("="*70)

    start = time.perf_counter()

    # Step 1: Generate data (all in memory)
    print("Generating 1M records in memory...")
    data = []
    for i in range(1_000_000):
        data.append({
            "id": i,
            "name": f"user_{i}",
            "value": i * 2.5,
            "tags": ["tag1", "tag2", "tag3"]
        })

    print(f"  - Generated {len(data):,} records")
    estimated_memory = len(data) * 150  # Rough estimate: ~150 bytes per record
    print(f"  - Estimated memory: ~{estimated_memory / 1024 / 1024:.1f} MB")

    # Step 2: Transform data (all in memory)
    print("Transforming records in memory...")
    transformed = []
    for record in data:
        transformed.append({
            "id": record["id"],
            "value": record["value"] * 10,
            "processed": True
        })

    print(f"  - Transformed {len(transformed):,} records")

    # Step 3: Aggregate
    print("Aggregating results...")
    total_value = sum(r["value"] for r in transformed)
    max_value = max(r["value"] for r in transformed)

    duration = time.perf_counter() - start

    print(f"\nResults:")
    print(f"  - Total value: {total_value:,.0f}")
    print(f"  - Max value: {max_value:,.0f}")
    print(f"  - Duration: {duration:.2f}s")
    print(f"  - Peak memory: ~{estimated_memory * 2 / 1024 / 1024:.1f} MB (2x for transform)")


# ==============================================================================
# Approach 2: Generator Streaming (Low Memory Usage)
# ==============================================================================

def generator_streaming_example():
    """Generator-based streaming approach - constant memory usage."""
    print("\n" + "="*70)
    print("Approach 2: Generator Streaming (Constant Memory)")
    print("="*70)

    start = time.perf_counter()

    print("Executing streaming pipeline...")

    # Step 1: Extract - Generate 1M records as a stream
    @generator_task(format="jsonl")
    def extract_data():
        """Extract step - generates 1M records as a stream."""
        print("  Streaming 1M records to disk...")
        for i in range(1_000_000):
            yield {
                "id": i,
                "name": f"user_{i}",
                "value": i * 2.5,
                "tags": ["tag1", "tag2", "tag3"]
            }

    extract_path = extract_data()

    # Step 2: Transform - Process records one at a time
    @generator_task(format="jsonl")
    def transform_data():
        """Transform step - processes records one at a time."""
        print("  Transforming records (streaming)...")

        # Read lazily - only one record in memory at a time
        reader = StreamReader(extract_path)
        for record in reader:
            yield {
                "id": record["id"],
                "value": record["value"] * 10,
                "processed": True
            }

    transform_path = transform_data()

    # Step 3: Aggregate - Load and aggregate results by streaming
    def aggregate_data():
        """Load step - aggregate results by streaming."""
        print("  Aggregating results (streaming)...")

        # Read and aggregate without loading all into memory
        reader = StreamReader(transform_path)

        total_value = 0
        max_value = 0
        count = 0

        for record in reader:
            total_value += record["value"]
            max_value = max(max_value, record["value"])
            count += 1

        return {
            "total_value": total_value,
            "max_value": max_value,
            "count": count
        }

    result = aggregate_data()

    duration = time.perf_counter() - start

    print(f"\nResults:")
    print(f"  - Total value: {result['total_value']:,.0f}")
    print(f"  - Max value: {result['max_value']:,.0f}")
    print(f"  - Record count: {result['count']:,}")
    print(f"  - Duration: {duration:.2f}s")
    print(f"  - Memory usage: ~50 MB (constant)")


# ==============================================================================
# Comparison Summary
# ==============================================================================

def print_comparison():
    """Print comparison summary."""
    print("\n" + "="*70)
    print("COMPARISON SUMMARY")
    print("="*70)

    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Metric              â”‚ In-Memory        â”‚ Generator Stream   â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ Records Processed   â”‚ 1,000,000        â”‚ 1,000,000          â”‚")
    print("â”‚ Peak Memory         â”‚ ~285 MB          â”‚ ~50 MB             â”‚")
    print("â”‚ Memory Complexity   â”‚ O(n)             â”‚ O(1)               â”‚")
    print("â”‚ Scalability         â”‚ Limited by RAM   â”‚ Unlimited          â”‚")
    print("â”‚ Can handle 100M?    â”‚ No (28 GB RAM)   â”‚ Yes (50 MB RAM)    â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    print("\nğŸ’¡ Key Benefits of Generator Streaming:")
    print("  1. Constant memory usage regardless of dataset size")
    print("  2. Can process datasets larger than available RAM")
    print("  3. Starts processing immediately (no waiting for full load)")
    print("  4. Works with 100M+ row datasets on modest hardware")
    print("  5. Natural backpressure - downstream controls pace")

    print("\nğŸ“Š When to Use Generator Streaming:")
    print("  âœ“ Dataset size > 1GB")
    print("  âœ“ Processing can be done record-by-record")
    print("  âœ“ ETL pipelines with large intermediate results")
    print("  âœ“ Database exports/imports")
    print("  âœ“ Log processing and analytics")

    print("\nğŸ“ When In-Memory is OK:")
    print("  âœ“ Dataset size < 100MB")
    print("  âœ“ Need random access to data")
    print("  âœ“ Multiple passes over data required")
    print("  âœ“ Complex transformations needing full dataset")


# ==============================================================================
# Mini Example: Simple Generator Usage
# ==============================================================================

def mini_example():
    """Quick example showing basic usage."""
    print("\n" + "="*70)
    print("MINI EXAMPLE: Basic Generator Task")
    print("="*70)

    from queuack import StreamReader, generator_task

    @generator_task(format="jsonl")
    def generate_numbers():
        """Generate numbers 1 to 100."""
        for i in range(1, 101):
            yield {"number": i, "square": i ** 2}

    # Call the function - returns path to file
    output_path = generate_numbers()
    print(f"\nGenerated file: {output_path}")

    # Read back and sum
    reader = StreamReader(output_path)
    total = sum(item["square"] for item in reader)

    print(f"Sum of squares 1-100: {total}")
    print(f"Expected: {sum(i**2 for i in range(1, 101))}")

    print("\nâœ“ That's it! Generator tasks make streaming simple.")


# ==============================================================================
# Main
# ==============================================================================

def main():
    """Run all examples."""
    print("\n" + "ğŸ¦†"*35)
    print("  Queuack Generator Streaming Demo")
    print("  Processing 1 Million Records")
    print("ğŸ¦†"*35)

    # Run mini example first
    mini_example()

    # Run both approaches
    in_memory_example()
    generator_streaming_example()

    # Show comparison
    print_comparison()

    print("\n" + "="*70)
    print("Demo complete! Check out the test suite for more examples.")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
